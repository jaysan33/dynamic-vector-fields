{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cec776-df8e-452f-add7-6b233e582f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "'''\n",
    "The code below works with a velocity vector field across discrete time steps\n",
    "\n",
    "The code will model the evoluation of a particle dropped randomly into the vector field\n",
    "\n",
    "Code utilizes a utilities file \"Field_utils\" that contains helper functions\n",
    "\n",
    "Your data should be parsed into two arrays:  1) x-direction velocities for each grid point across time steps; 2) y-direction velocities for\n",
    "each grid point across time steps\n",
    "\n",
    "e.g. a 504 x 505 x 100 array of velocities in the x-direction (horizontal direction)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#call in the x and y directional fields from your data source\n",
    "x_veloc,y_veloc = .....\n",
    "\n",
    "#vector field shape\n",
    "# e.g. a 504 x 555 x 100 grid would lead to x_shape = x_veloc.shape[1] = 555; y_shape = x_veloc.shape[0] = 504; time_shape = x_veloc.shape[2]\n",
    "\n",
    "\n",
    "time_delta = ...  #units corresponding to the time differential between successive vector fields\n",
    "num_steps = ...  #number of directional velocity steps taken within each time interval\n",
    "h = time_delta/num_steps\n",
    "\n",
    "simulations = ...\n",
    "\n",
    "\n",
    "    \n",
    "for j in range(simulations): \n",
    "    position_history_x = []  #set up a list to keep track of the positions across time\n",
    "    position_history_y = []  \n",
    "    random_start = np.array([np.random.rand()*y_shape, np.random.rand()*x_shape]) #shape (2,)\n",
    "    current_position = random_start\n",
    "    position_history_x.append(current_position[0])  \n",
    "    position_history_y.append(current_position[1])\n",
    "    \n",
    "    for t in range(time_shape):\n",
    "        n = 1   \n",
    "        while n*h <=time_delta:\n",
    "            x,y = current_position[0],current_position[1]\n",
    "            active_x,active_y = Field_utils.active_grid(x,y)\n",
    "            new_x = max(min(x_veloc[active_x,active_y,t]*h + x,(y_shape -1),0)\n",
    "            new_y = max(min(y_veloc[active_x,active_y,t]*h + y,(x_shape -1),0)\n",
    "            current_position = np.array([new_x,new_y])\n",
    "            position_history_x.append(current_position[0])\n",
    "            position_history_y.append(current_position[1])\n",
    "            n = n+1\n",
    "            \n",
    "    plt.plot(position_history_x,position_history_y,markevery=[0],marker=\"x\",markersize=6)\n",
    "   \n",
    "    \n",
    "plt.axis([0,y_shape,0,x_shape])\n",
    "plt.gca().set_aspect(1)\n",
    "plt.title(\"Simulations of Flows for a uniformly \\nrandomly placed particle/object\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b115b-5bd7-440c-a374-a7a48c50b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next, we can  parametrize a Gaussian process to model the time dynamics of the velocity vector field.  This\n",
    "will ultimately allow one to model varying time intervals for the vector field\n",
    "Step (1):  Assume a Gaussian process for the x and y directional vectors over time\n",
    "Step (2):  Assume a Radial Basis Function Kernel (RBF) for the Covariance dynamics\n",
    "Step (3):  Use cross-validation to choose the RBF paremeters for your model\n",
    "\n",
    "The outputs of this analysis will be conditional means and conditional covariance matrices that can be used\n",
    "for inference over different time intervals\n",
    "\n",
    "Below, I show how this can be done for a single grid point\n",
    "'''\n",
    "\n",
    "#pick a location of your liking from your vector field\n",
    "#example:\n",
    "loc = [95, 335]\n",
    "\n",
    "u = np.zeros((time_shape,2))  #first column is index, second column is vector\n",
    "v = np.zeros((time_shape,2))  \n",
    "\n",
    "#initiate two arrays holding time index and velocity vector\n",
    "for i in range(time_shape):\n",
    "    u[i,0] = i\n",
    "    u[i,1] = x_veloc[loc[0],loc[1],i]\n",
    "    v[i,0] = i\n",
    "    v[i,1] = y_veloc[loc[0],loc[1],i]\n",
    "\n",
    "\n",
    "#Set up initial parameters for RBF and cross validation:\n",
    "#example:\n",
    "sigma = .05  #for radial basis kernel\n",
    "L = 15  #for radiol basis kernel\n",
    "r = .001\n",
    "k = 10  #k-fold cross validation...will give time_shape/10 = 10 data points for testing\n",
    "\n",
    "#will need means for the stochastic x and y vector variables...here will use the average across the time_shape data points\n",
    "mean_u = np.mean(u[:,1])\n",
    "mean_v = np.mean(v[:,1])\n",
    "\n",
    "#choose your hyperparameters for cross-validation\n",
    "#example:\n",
    "sigmas = np.linspace(.03,.1,5)\n",
    "Ls = np.linspace(90,150,20)\n",
    "\n",
    "##set up a table for results\n",
    "results = np.zeros((Ls.size,sigmas.size))\n",
    "\n",
    "#first do for the u-vectors\n",
    "for l in Ls:\n",
    "    for s in sigmas:\n",
    "        cum_likelihood = 1\n",
    "\n",
    "        #first set up calcs for the u vectors \n",
    "        for i in range(k):\n",
    "            beg = int(100/k*i)\n",
    "            end = int(100/k*(i+1))\n",
    "            valid = np.vstack((u[0:beg,:],u[end:,:]))  \n",
    "            test = u[beg:end,:]\n",
    "            #now I have the validation and test partitions, with the first columns being the indices/distance metric\n",
    "            cov11 = Field_utils.kernel_1(test,s,l)\n",
    "            cov22 = Field_utils.kernel_1(valid,s,l)\n",
    "            cov12 = Field_utils.kernel_2(test,valid,s,l)\n",
    "            #now carry out the conditional mean and variance calculations:\n",
    "            cond_mean = Field_utils.cond_mean(mean_u,mean_u,cov12,cov22,r,valid[:,1].reshape(-1,1))\n",
    "            cond_var = Field_utils.cond_var(cov11,cov12,cov22,r)\n",
    "            #next, compute the likelihood of the test data on the conditional distribution\n",
    "            likelihood = multivariate_normal.pdf(test[:,1].reshape(-1,),mean=cond_mean.reshape(-1,),cov=cond_var)\n",
    "            cum_likelihood = cum_likelihood*likelihood\n",
    "                 \n",
    "        results[np.argwhere(Ls==l),np.argwhere(sigmas==s)] = round(np.log(cum_likelihood),2)\n",
    "\n",
    "#now for the v-vectors\n",
    "\n",
    "sigmas_v = np.linspace(.03,.1,5)\n",
    "Ls_v = np.linspace(90,150,20)\n",
    "\n",
    "##set up a table for results\n",
    "results_v = np.zeros((Ls_v.size,sigmas_v.size))\n",
    "\n",
    "for l in Ls_v:\n",
    "    for s in sigmas_v:\n",
    "        cum_likelihood = 1\n",
    "\n",
    "        #first set up calcs for the v vectors \n",
    "        for i in range(k):\n",
    "            beg = int(100/k*i)\n",
    "            end = int(100/k*(i+1))\n",
    "            valid = np.vstack((v[0:beg,:],v[end:,:]))  \n",
    "            test = v[beg:end,:]\n",
    "            #now I have the validation and test partitions, with the first columns being the indices/distance metric\n",
    "            cov11 = Field_utils.kernel_1(test,s,l)\n",
    "            cov22 = Field_utils.kernel_1(valid,s,l)\n",
    "            cov12 = Field_utils.kernel_2(test,valid,s,l)\n",
    "            #now carry out the conditional mean and variance calculations:\n",
    "            cond_mean = Field_utils.cond_mean(mean_v,mean_v,cov12,cov22,r,valid[:,1].reshape(-1,1))\n",
    "            cond_var = Field_utils.cond_var(cov11,cov12,cov22,r)\n",
    "            #next, compute the likelihood of the test data on the conditional distribution\n",
    "            likelihood = multivariate_normal.pdf(test[:,1].reshape(-1,),mean=cond_mean.reshape(-1,),cov=cond_var)\n",
    "            cum_likelihood = cum_likelihood*likelihood\n",
    "            \n",
    "        \n",
    "        results_v[np.argwhere(Ls_v==l),np.argwhere(sigmas_v==s)] = round(np.log(cum_likelihood),2)\n",
    "\n",
    "\n",
    "results_pd = pd.DataFrame(results,index=np.round(Ls,0),columns=sigmas)\n",
    "results_pd.index.name = 'L value'\n",
    "results_pd.columns.name = 'Sigma'\n",
    "\n",
    "\n",
    "results_pd.style.background_gradient(axis= None, cmap='Greens',vmin=80.0)\n",
    "\n",
    "\n",
    "results_pd_v = pd.DataFrame(results_v,index=np.round(Ls_v,0),columns=sigmas_v)\n",
    "results_pd_v.index.name = 'L value'\n",
    "results_pd_v.columns.name = 'Sigma'\n",
    "\n",
    "results_pd_v.style.background_gradient(axis= None, cmap='Greens',vmin=70.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41c89e-5326-4ab7-91f8-c2aea41e4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can also utilize SKLearn's GaussianProcessorRegressor functionality\n",
    "to determine the appropriate kernel parameters.  Use the below code and compare the\n",
    "Kernel output parameters to those you derived via cross-validation\n",
    "\n",
    "'''\n",
    "import sklearn as sk\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "#instantiate the regressor objects\n",
    "GPR_u = 1**2*GaussianProcessRegressor()  #placing the 1**2 in front allows the GP regressor to tune for sigma\n",
    "GPR_v = 1**2*GaussianProcessRegressor()\n",
    "\n",
    "#fit to your data\n",
    "GPR_u.fit(u[:,0].reshape(-1,1),u[:,1].reshape(-1,1))\n",
    "GPR_v.fit(v[:,0].reshape(-1,1),v[:,1].reshape(-1,1))\n",
    "\n",
    "GPR_u.kernel_\n",
    "GPR_v.kernel_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852595f-5def-46fb-8bdb-9d6a614fbe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next, we can utilize our GP modelling to interpolate the time dynamics of the vector field\n",
    "\n",
    "Step (1):  Determine what time intervals you want to analyze; e.g. if your velocity field data has 100 time stamps where\n",
    "measurement intervals represent 1 hour, then you can fine tune your model to interpolate the dynamics of the velocity field for \n",
    "10 minute intervals for example.\n",
    "\n",
    "Step (2):  Use your chosen kernel hyperparameters from the above analysis\n",
    "\n",
    "'''\n",
    "\n",
    "#The code below assumes your original vector field data was measured 100 different times, i.e. 100 time stamps\n",
    "#Your goal was to interpolate the vector field to obtain a total of 300 time stamps:  the 100 original plus 200 interpolated\n",
    "#you can change these  below by adjusting the code \n",
    "\n",
    "u_L_opt = ...\n",
    "u_sigma_opt = ...\n",
    "v_L_opt = ...\n",
    "v_sigma_opt = ...\n",
    "\n",
    "\n",
    "unobs_vector = np.zeros((200))\n",
    "for i in range(100):\n",
    "    unobs_vector[i*2] = i + 1.3333\n",
    "    unobs_vector[(i*2+1)] = i + 1.6666\n",
    "\n",
    "\n",
    "obs_vector = np.linspace(1,100,100)\n",
    "\n",
    "size_unobs = unobs_vector.shape[0]\n",
    "size_obs = obs_vector.shape[0]\n",
    "cov_unobs_u = np.zeros((size_unobs,size_unobs))\n",
    "cov_unobs_v = np.zeros((size_unobs,size_unobs))\n",
    "cov_obs_u = np.zeros((size_obs,size_obs))\n",
    "cov_obs_v = np.zeros((size_obs,size_obs))\n",
    "\n",
    "for i in range(size_unobs):\n",
    "        for j in range(size_unobs):\n",
    "            cov_unobs_u[i,j] = Field_utils.kernel_func(unobs_vector[i],unobs_vector[j],u_sigma_opt,u_L_opt)\n",
    "\n",
    "for i in range(size_unobs):\n",
    "        for j in range(size_unobs):\n",
    "            cov_unobs_v[i,j] = Field_utils.kernel_func(unobs_vector[i],unobs_vector[j],v_sigma_opt,v_L_opt)\n",
    "\n",
    "for i in range(size_obs):\n",
    "        for j in range(size_obs):\n",
    "            cov_obs_u[i,j] = Field_utils.kernel_func(obs_vector[i],obs_vector[j],u_sigma_opt,u_L_opt)\n",
    "\n",
    "for i in range(size_obs):\n",
    "        for j in range(size_obs):\n",
    "            cov_obs_v[i,j] = Field_utils.kernel_func(obs_vector[i],obs_vector[j],v_sigma_opt,v_L_opt)\n",
    "\n",
    "\n",
    "## now you have the square covariance matrix for my unobserved flows and my observed flows\n",
    "\n",
    "#compute cross covariances between obs and unobs\n",
    "\n",
    "cross_cov_u = np.zeros((size_unobs,size_obs))\n",
    "cross_cov_v = np.zeros((size_unobs,size_obs))\n",
    "\n",
    "for i in range(size_unobs):\n",
    "    for j in range(size_obs):\n",
    "        cross_cov_u[i,j] = Field_utils.kernel_func(unobs_vector[i],obs_vector[j],u_sigma_opt,u_L_opt)\n",
    "\n",
    "for i in range(size_unobs):\n",
    "    for j in range(size_obs):\n",
    "        cross_cov_v[i,j] = Field_utils.kernel_func(unobs_vector[i],obs_vector[j],v_sigma_opt,v_L_opt)\n",
    "\n",
    "\n",
    "#compute conditional means and conditional variances for the u and v-velocities\n",
    "#the blue dots on the plot will be the observed data, while the red dots will be the interpolated data\n",
    "\n",
    "u_cond_mean_unobs = HW5_utils.cond_mean(mean_u,mean_u,cross_cov_u,cov_obs_u,r,u[:,1])\n",
    "print(u_cond_mean_unobs.shape)\n",
    "u_cond_cov_unobs = HW5_utils.cond_var(cov_unobs_u,cross_cov_u,cov_obs_u,r)\n",
    "print(u_cond_cov_unobs.shape)\n",
    "\n",
    "v_cond_mean_unobs = HW5_utils.cond_mean(mean_v,mean_v,cross_cov_v,cov_obs_v,r,v[:,1])\n",
    "print(v_cond_mean_unobs.shape)\n",
    "v_cond_cov_unobs = HW5_utils.cond_var(cov_unobs_v,cross_cov_v,cov_obs_v,r)\n",
    "\n",
    "\n",
    "#set up a variance vector for each unobs point\n",
    "u_var_unobs_top = np.zeros((200))\n",
    "u_var_unobs_bottom = np.zeros((200))\n",
    "for i in range(200):\n",
    "    u_var_unobs_top[i] = u_cond_mean_unobs[i,0] + np.sqrt(u_cond_cov_unobs[i,i])*3\n",
    "    u_var_unobs_bottom[i] = u_cond_mean_unobs[i,0] - np.sqrt(u_cond_cov_unobs[i,i])*3\n",
    "    \n",
    "\n",
    "plt.plot(unobs_vector.reshape(-1,1),u_var_unobs_top.reshape(-1,1),color='0.8')\n",
    "plt.plot(unobs_vector.reshape(-1,1),u_var_unobs_bottom.reshape(-1,1),color='0.8')\n",
    "plt.fill_between(unobs_vector,u_var_unobs_top,u_var_unobs_bottom,color='0.8')\n",
    "plt.scatter(unobs_vector.reshape(-1,1),u_cond_mean_unobs,label=\"conditional mean\",color='red')\n",
    "plt.scatter(obs_vector.reshape(-1,1),u[:,1],label=\"observed u_velocities\",color='blue')\n",
    "            \n",
    "\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "v_var_unobs_top = np.zeros((200))\n",
    "v_var_unobs_bottom = np.zeros((200))\n",
    "\n",
    "for i in range(200):\n",
    "    v_var_unobs_top[i] = v_cond_mean_unobs[i,0] + np.sqrt(v_cond_cov_unobs[i,i])*3\n",
    "    v_var_unobs_bottom[i] = v_cond_mean_unobs[i,0] - np.sqrt(v_cond_cov_unobs[i,i])*3\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(unobs_vector.reshape(-1,1),v_var_unobs_top.reshape(-1,1),color='0.8')\n",
    "plt.plot(unobs_vector.reshape(-1,1),v_var_unobs_bottom.reshape(-1,1),color='0.8')\n",
    "plt.fill_between(unobs_vector,v_var_unobs_top,v_var_unobs_bottom,color='0.8')\n",
    "plt.scatter(unobs_vector.reshape(-1,1),v_cond_mean_unobs,label=\"conditional mean\",color='red')\n",
    "plt.scatter(obs_vector.reshape(-1,1),v[:,1],label=\"observed v_velocities\",color='blue')\n",
    "\n",
    "plt.legend()\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
